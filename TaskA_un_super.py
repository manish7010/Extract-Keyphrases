# -*- coding: utf-8 -*-
"""TaskA-UNSuper.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1N5mmebQfRguogrVNaHPg-r2EAAxf8Fw2
"""

import pickle

from google.colab import files
uploaded = files.upload()

with open('test-text.pkl', 'rb') as f:
  text= pickle.load(f)

with open('test-anno.pkl', 'rb') as f:
  annotation= pickle.load(f)

len(text)

len(annotation)

text[0]

annotation[0][0][3]







!pip install git+https://github.com/boudinfl/pke.git

!pip install keybert

!pip install keyphrase-vectorizers

from keybert import KeyBERT
#from keyphrase_vectorizers import KeyphraseCountVectorizer

kw_model = KeyBERT()

#docs = ['Within a coalescence approach as successfully applied earlier in the light-quark sector, we have evaluated transverse-momentum dependencies of charmed hadrons in central heavy-ion reactions at RHIC. For the charm-quark distributions at hadronization we have considered two limiting scenarios, i.e., no reinteractions (using spectra from PYTHIA) and complete thermalization with transverse flow of the bulk matter. The resulting J/ψ (mT-)spectra differ in slope by up to a factor of\xa02 (harder for pQCD c-quarks), and the integrated yield is about a factor of\xa03 larger in the thermal case. For D-mesons, we found that the difference in the slope parameters of the pT-spectra in the two scenarios is less pronounced, but their elliptic flow is about a factor of\xa02 larger for pT⩾1.5\xa0GeV in the thermalized case. The elliptic flow pattern of D-mesons was found to be essentially preserved in the single-electron decay spectra, rendering the latter a very promising observable to address the strength of charm reinteractions in the QGP. The present study can be straightforwardly generalized to charmed baryons (Λc), which may serve as a complimentary probe for charm-quark reinteractions in the QGP.\n']

docs = [text[0]]

kw_model.extract_keywords(docs=docs, keyphrase_ngram_range=(1,2))

keywords = kw_model.extract_keywords(docs, keyphrase_ngram_range=(1, 2),
                     nr_candidates=20, top_n=15)

keywords

def predict_keyphrases(text):
  pre_kp = []
  for i in range(len(text)):
    keyphrases = kw_model.extract_keywords(text[i], keyphrase_ngram_range=(1, 2),
                     nr_candidates=20, top_n=15)
    pre_kp.append(keyphrases)
  return pre_kp

predicted = predict_keyphrases(text)

predicted[4]

def EM_score(annotation,predicted):
  score = 0
  for i in range(len(annotation)):
    for k in range(len(predicted[i])):
      for j in range(len(annotation[i])):
        #print(k)
        #if j < k: 
        if (annotation[i][j][3] == predicted[i][k][0]):
          score += 1 
  return score/1500

EM = EM_score(annotation,predicted)

# For KeyBert
EM

import pke

extractor = pke.unsupervised.TextRank()
extractor.load_document(input=text[6],language='en',normalization=None)
extractor.candidate_weighting(window=2,top_percent=0.66)

extractor.candidate_weighting(window=2,
                              #pos=pos,
                              top_percent=0.66)

keyphrases = extractor.get_n_best(n=15)

keyphrases[0][0]





def predict_keyphrases(text):
  pre_kp = []
  for i in range(len(text)):
    extractor = pke.unsupervised.TextRank()
    extractor.load_document(input=text[i],language='en',normalization=None)
    extractor.candidate_weighting(window=2,top_percent=0.66)
    extractor.candidate_weighting(window=2,
                              #pos=pos,
                              top_percent=0.66)
    keyphrases = extractor.get_n_best(n=15)
    pre_kp.append(keyphrases)
  return pre_kp

predicted = predict_keyphrases(text)

predicted[5][4]

def EM_score(annotation,predicted):
  score = 0
  for i in range(len(annotation)):
    for k in range(len(predicted[i])):
      for j in range(len(annotation[i])):
        #print(k)
        #if j < k: 
        if (annotation[i][j][3] == predicted[i][k][0]):
          score += 1 
  return score/1500

EM = EM_score(annotation,predicted)

#For Text Rank
EM





def predict_keyphrases(text):
  pre_kp = []
  for i in range(len(text)):
    extractor = pke.unsupervised.TopicRank() 
    extractor.load_document(text[i])
    extractor.candidate_selection()
    extractor.candidate_weighting()
    keyphrases = extractor.get_n_best(n=15, stemming=False)

    pre_kp.append(keyphrases)
  return pre_kp

predicted = predict_keyphrases(text)

predicted[0]

EM = EM_score(annotation,predicted)

# For Topic Rank
EM



def predict_keyphrases(text):
  pre_kp = []
  for i in range(len(text)):
    extractor = pke.unsupervised.YAKE()
    #stoplist = stopwords.words('english')
    extractor.load_document(input=text[i],language='en',
                        normalization=None)
                        #stoplist=stoplist)
    
    extractor.candidate_selection(n=3)
    window = 2
    use_stems = False # use stems instead of words for weighting
    extractor.candidate_weighting(window=window,
                              use_stems=use_stems)
    threshold = 0.6
    keyphrases = extractor.get_n_best(n=15, threshold=threshold)

    pre_kp.append(keyphrases)
  return pre_kp

predicted = predict_keyphrases(text)

EM = EM_score(annotation,predicted)

# For Yake
EM











